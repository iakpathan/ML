{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Wrapper Methods: Unlike filter which uses statistical methods to select features,Wrapper treats model as as a black box and evaluate feature combinations based on their impact on model's performance.\n",
        "Process: Subset generation -> model train on that subset->evaluate performance->Selection based on performance->repeat until desired number of features or performance is achieved.\n",
        "Strategies in Wrapper methods:\n",
        "1)Forward selection:start with empty set , select a model, by adding a feature train model and evaluate performance\n",
        "2)Backward Selection: start with entire dataset, select a model, calc performance step by step remove features and calc performance.\n",
        "3)Recursive Feature Elimination (RFE):Rank features and eliminate least imp recursively\n",
        "Feature Importance: It uses the estimator's intrinsic feature importance method (like coefficients for linear models or feature importance for tree-based models) to score every feature.\n",
        "Disadvantages:Computationally costlier.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "A5_yqPHL4Dxx",
        "outputId": "6face45e-4f07-47c9-9bf1-63f4e8c2d007"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nWrapper Methods: Unlike filter which uses statistical methods to select features,Wrapper treats model as as a black box and evaluate feature combinations based on their impact on model's performance.\\nProcess: Subset generation -> model train on that subset->evaluate performance->Selection based on performance->repeat until desired number of features or performance is achieved.\\nStrategies in Wrapper methods:\\n1)Forward selection:start with empty set , select a model, by adding a feature train model and evaluate performance \\n2)Backward Selection: start with entire dataset, select a model, calc performance step by step remove features and calc performance.\\n3)Recursive Feature Elimination (RFE):Rank features and eliminate least imp recursively\\nFeature Importance: It uses the estimator's intrinsic feature importance method (like coefficients for linear models or feature importance for tree-based models) to score every feature.\\nDisadvantages:Computationally costlier.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1dxHR-Q258_",
        "outputId": "2e83268f-8a6d-4001-fb1a-953d01e393c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial features: ['pclass', 'age', 'sibsp', 'parch', 'fare', 'sex_male', 'embarked_Q', 'embarked_S']\n",
            "Training set shape: (623, 8)\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3738674249.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['age'].fillna(data['age'].median(), inplace=True)\n",
            "/tmp/ipython-input-3738674249.py:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n"
          ]
        }
      ],
      "source": [
        "'''Dataset and model(estimator) selection and processing'''\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
        "from sklearn.feature_selection import RFE\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Load the Titanic dataset\n",
        "try:\n",
        "    df = sns.load_dataset('titanic')\n",
        "except:\n",
        "    # Fallback for environments without seaborn data\n",
        "    print(\"Could not load Titanic from seaborn; please ensure it's installed.\")\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "# 2. Select Titanic-like features and the target\n",
        "# We use Pclass, Sex, Age, SibSp, Parch, Fare, and Embarked\n",
        "data = df[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked', 'survived']].copy()\n",
        "\n",
        "# 3. Handle Missing Values\n",
        "# Fill 'Age' with the median\n",
        "data['age'].fillna(data['age'].median(), inplace=True)\n",
        "# Fill 'Embarked' with the most frequent value (mode)\n",
        "data['embarked'].fillna(data['embarked'].mode()[0], inplace=True)\n",
        "\n",
        "# 4. Handle Categorical Variables (One-Hot Encoding)\n",
        "data = pd.get_dummies(data, columns=['sex', 'embarked'], drop_first=True)\n",
        "\n",
        "# 5. Define Features (X) and Target (y)\n",
        "X = data.drop('survived', axis=1)\n",
        "y = data['survived']\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Initialize the model (estimator)\n",
        "log_reg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "print(f\"Initial features: {list(X.columns)}\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 2. Forward Selection ---\")\n",
        "\n",
        "# Initialize SFS with forward=True\n",
        "sfs_forward = SFS(\n",
        "    estimator=log_reg,\n",
        "    k_features='best', # Find the optimal number of features between 1 and 8\n",
        "    forward=True,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit SFS to the training data\n",
        "sfs_forward = sfs_forward.fit(X_train, y_train)\n",
        "\n",
        "# Get the best feature subset\n",
        "selected_features_fs = list(sfs_forward.k_feature_names_)\n",
        "\n",
        "print(f\"Optimal features count: {sfs_forward.k_feature_idx_}\")\n",
        "print(f\"Best features: {selected_features_fs}\")\n",
        "print(f\"Cross-validation score: {sfs_forward.k_score_:.4f}\")\n",
        "\n",
        "# Train the final model and test\n",
        "log_reg.fit(X_train[selected_features_fs], y_train)\n",
        "y_pred_fs = log_reg.predict(X_test[selected_features_fs])\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_fs):.4f}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPugvBsV26qa",
        "outputId": "052b73c9-3276-4c26-867d-c9e338be74a2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 2. Forward Selection ---\n",
            "Optimal features count: (0, 1, 2, 5)\n",
            "Best features: ['pclass', 'age', 'sibsp', 'sex_male']\n",
            "Cross-validation score: 0.8057\n",
            "Test Accuracy: 0.7948\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"--- 3. Backward Elimination ---\")\n",
        "\n",
        "# Initialize SFS with forward=False (Backward Elimination)\n",
        "sfs_backward = SFS(\n",
        "    estimator=log_reg,\n",
        "    k_features='best', # Find the optimal number of features\n",
        "    forward=False,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit SBS to the training data\n",
        "sfs_backward = sfs_backward.fit(X_train, y_train)\n",
        "\n",
        "# Get the best feature subset\n",
        "selected_features_bs = list(sfs_backward.k_feature_names_)\n",
        "\n",
        "print(f\"Optimal features count: {sfs_backward.k_feature_idx_}\")\n",
        "print(f\"Best features: {selected_features_bs}\")\n",
        "print(f\"Cross-validation score: {sfs_backward.k_score_:.4f}\")\n",
        "\n",
        "# Train the final model and test\n",
        "log_reg.fit(X_train[selected_features_bs], y_train)\n",
        "y_pred_bs = log_reg.predict(X_test[selected_features_bs])\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_bs):.4f}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qT2niyJ83HhQ",
        "outputId": "21d28c73-b8b6-42cf-f7a9-c7e0e4ac84d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 3. Backward Elimination ---\n",
            "Optimal features count: (0, 1, 2, 5)\n",
            "Best features: ['pclass', 'age', 'sibsp', 'sex_male']\n",
            "Cross-validation score: 0.8057\n",
            "Test Accuracy: 0.7948\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"--- 4. Recursive Feature Elimination (RFE) ---\")\n",
        "# We'll aim to select the best 4 features (just an arbitrary choice for RFE)\n",
        "num_features_rfe = 4\n",
        "\n",
        "# Initialize RFE\n",
        "rfe_selector = RFE(\n",
        "    estimator=log_reg,\n",
        "    n_features_to_select=num_features_rfe,\n",
        "    step=1 # Remove 1 feature at each iteration\n",
        ")\n",
        "\n",
        "# Fit RFE to the training data\n",
        "rfe_selector = rfe_selector.fit(X_train, y_train)\n",
        "\n",
        "# Get the selected features\n",
        "selected_mask = rfe_selector.support_\n",
        "selected_features_rfe = X.columns[selected_mask].tolist()\n",
        "\n",
        "print(f\"Best features (k={num_features_rfe}): {selected_features_rfe}\")\n",
        "\n",
        "# Train the final model and test\n",
        "log_reg.fit(X_train[selected_features_rfe], y_train)\n",
        "y_pred_rfe = log_reg.predict(X_test[selected_features_rfe])\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred_rfe):.4f}\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPN17KXd3Mj9",
        "outputId": "b48566aa-5a5b-4a22-a0df-8726453fedd6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 4. Recursive Feature Elimination (RFE) ---\n",
            "Best features (k=4): ['pclass', 'sex_male', 'embarked_Q', 'embarked_S']\n",
            "Test Accuracy: 0.7761\n",
            "\n",
            "==================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PZf0l1g83PAI"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}